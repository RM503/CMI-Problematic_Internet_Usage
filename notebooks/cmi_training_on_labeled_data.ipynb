{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMI - predictions on labeled data with ensemble voting\n",
    "Here, I train only the subset of ```train.csv``` that contains the ```sii``` labels using boosted regressors (XGBoost, LightGBM and CatBoost). Even though the ```sii``` label is categorical, the ordinal nature of the label means that the class probability thresholds become important and the classification task can actually be approached using regression, with a rounding-off mechanism for making the final predictions. This notebook contains the following -\n",
    "1. **Time-series statistics extraction:** Simple statistics from the time-series parquet files are calculated, extracted and appended to the ```train.csv``` and ```test.csv``` files. The time-series statistics can be parallelized in many ways. Here, it is done using ```joblib```.\n",
    "2. **Cross-validation:** The data is trained and validated in a ```StratifiedKFold``` training and validation loop. I make sure to perform the required data preprocessing steps, imputation and scaling (if required), inside this loop to prevent *data leakage*. In this way, the data preprocessing pipeline can be ```fit_transform()```'d on the training set, while the validation and test sets are only ```transform()```'d. Inside the cross-validation loop, the different regressor models are fitted and the out-of-fold (OOF) predictions are calculated. Importantly, optimal thresholds for rounding-off these predictions to ordinal variables is performed.\n",
    "3. **Hyperparameter Optimization:** The hyperparameter optimization was performed using ```Optuna``` and they are only stated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import clone \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('xtick',labelsize=12)\n",
    "plt.rc('ytick',labelsize=12)\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "The training and test data consist of ```.csv``` files containing tabular data of over $3,000$ participants. Furthermore, the dataset also contains actigraphy data stored in ```.parquet``` files for a certain subset of participants in the form of tabular time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './../child-mind-institute-problematic-internet-use/'\n",
    "\n",
    "train_df = pd.read_csv(folder_path + 'train.csv', index_col='id')\n",
    "test_df = pd.read_csv(folder_path + 'test.csv', index_col='id')\n",
    "data_dict = pd.read_csv(folder_path + 'data_dictionary.csv')\n",
    "\n",
    "timeseries_train_files = glob.glob(folder_path + 'series_train.parquet/*')\n",
    "timeseries_test_files = glob.glob(folder_path + 'series_test.parquet/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting time-series features and statistics\n",
    "The actigraphy data contains a wealth of information that can be extremely helpful at predicting problematic internet usage among children and adolescents. In particular, the time-series data contains the ```enmo``` feature, which is a generalized notion of the acceleration experienced by the user's hand. Statistics of such variables, especially during the weekends, can help strenghten the training dataset, allowing for more robust predictive modeling. The ```extract_timeseries_features()``` function has been adapted from Lennard Haupt's notebook in https://www.kaggle.com/code/lennarthaupts/cmi-detecting-problematic-digital-behavior. I plan on performing a different statistical analysis on the actigraphy data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timeseries_features(df):\n",
    "    ''' \n",
    "    Extracting time-series features from the actigraphy datasets.\n",
    "    Copied from Lennard Haupt's notebook in https://www.kaggle.com/code/lennarthaupts/cmi-detecting-problematic-digital-behavior\n",
    "    '''\n",
    "\n",
    "    # Converting time_of_day to hours; the time_of_day variable is expressed in nanoseconds\n",
    "    df[\"hours\"] = df[\"time_of_day\"] // (3_600 * 1_000_000_000)\n",
    "\n",
    "    # Define conditions for night, day, and no mask (full data); night is defined to be between hours 22 and 5\n",
    "    night = ((df[\"hours\"] >= 22) | (df[\"hours\"] <= 5))\n",
    "    day = ((df[\"hours\"] <= 20) & (df[\"hours\"] >= 7))\n",
    "    no_mask = np.ones(len(df), dtype=bool)\n",
    "    # Define weekend and last week conditions\n",
    "    weekend = (df[\"weekday\"] >= 6)\n",
    "    last_week = df[\"relative_date_PCIAT\"] >= (df[\"relative_date_PCIAT\"].max() - 7)\n",
    "    # Create additional weekend features\n",
    "    df[\"enmo_weekend\"] = df[\"enmo\"].where(weekend)\n",
    "    df[\"anglez_weekend\"] = df[\"anglez\"].where(weekend)\n",
    "    # Basic features \n",
    "    features = [\n",
    "        df[\"non-wear_flag\"].mean(),\n",
    "        df[\"battery_voltage\"].mean(),\n",
    "        df[\"battery_voltage\"].diff().mean(),\n",
    "        df[\"relative_date_PCIAT\"].tail(1).values[0]\n",
    "    ]\n",
    "    \n",
    "    # List of columns of interest and masks\n",
    "    keys = [\"enmo\", \"anglez\", \"light\", \"enmo_weekend\", \"anglez_weekend\"]\n",
    "    masks = [no_mask, night, day, last_week]\n",
    "    \n",
    "    # Helper function for feature extraction\n",
    "    def extract_stats(data):\n",
    "        return [\n",
    "            data.mean(), \n",
    "            data.std(), \n",
    "            data.max(), \n",
    "            data.min(), \n",
    "            data.kurtosis(), \n",
    "            data.skew(), \n",
    "            data.diff().mean(), \n",
    "            data.diff().std(), \n",
    "            data.diff().quantile(0.9), \n",
    "            data.diff().quantile(0.1)\n",
    "        ]\n",
    "    \n",
    "    # Iterate over keys and masks to generate the statistics\n",
    "    for key in keys:\n",
    "        for mask in masks:\n",
    "            filtered_data = df.loc[mask, key]\n",
    "            features.extend(extract_stats(filtered_data))\n",
    "\n",
    "    return features\n",
    "\n",
    "def process_file(filepath):\n",
    "    ''' \n",
    "    This function takes the filepath of a time-series parquet file and extracts the id number and the\n",
    "    statistics defined in extract_timeseries_features() function.\n",
    "    '''\n",
    "    df = pd.read_parquet(filepath)\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "\n",
    "    features = extract_timeseries_features(df)\n",
    "    id_num = filepath.split('=')[-1]\n",
    "\n",
    "    return features, id_num\n",
    "\n",
    "def extract_timeseries_id(filepath_array):\n",
    "    ''' \n",
    "    This function takes an array of filepaths and returns an array of id numbers.\n",
    "    '''\n",
    "    id_list = []\n",
    "    for n, folder in enumerate(filepath_array):\n",
    "        id = folder.split('=')[-1]\n",
    "        id_list.append(id)\n",
    "\n",
    "    return id_list \n",
    "\n",
    "def timeseries_features_df(filepath_array, n_jobs=8):\n",
    "    ''' \n",
    "    This function takes as input a filepath array and returns a dataframe with all the time-series statistics for each parquet file.\n",
    "    The extraction process is sped up using Python's joblib library, with n_jobs being an optional argument.\n",
    "    '''\n",
    "    print('Extracting features from time-series files.')\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(process_file)(f) for f in tqdm(filepath_array))\n",
    "\n",
    "    stats, indices = zip(*results)\n",
    "    df = pd.DataFrame(stats, columns=[f'stat_{i}' for i in range(len(stats[0]))])\n",
    "    df['id'] = indices\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time-series statistics from the train and test parquet files are extracted into ```train_timeseries``` and ```test_timeseries``` dataframes. These are then merged with the data from the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from time-series files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/996 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:38<00:00, 26.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from time-series files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1423.49it/s]\n"
     ]
    }
   ],
   "source": [
    "train_timeseries = timeseries_features_df(timeseries_train_files)\n",
    "test_timeseries = timeseries_features_df(timeseries_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_cols = train_timeseries.columns.tolist()\n",
    "timeseries_cols.remove('id')\n",
    "\n",
    "train_df = pd.merge(train_df, train_timeseries, how='left', on='id')\n",
    "test_df = pd.merge(test_df, test_timeseries, how='left', on='id')\n",
    "\n",
    "train_df.set_index('id', inplace=True)\n",
    "test_df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial preprocessing\n",
    "The different ```PCIAT``` columns are first removed from the training dataset. Then, I consider the subset of the data that has ```sii``` values associated with them. The numerical and categorical columns are then separated out. Here, the categorical columns are those features which are labeled as ```categorical int``` in ```data_dictionary.csv```. I will also remove any feature category that has 'Enrolment Season' or 'Season' in its descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = train_df.columns.tolist()\n",
    "test_cols = test_df.columns.tolist()\n",
    "\n",
    "drop_cols = list( set(train_cols) - set(test_cols) )\n",
    "drop_cols.remove('sii')\n",
    "\n",
    "train_df_subset = train_df[train_df.loc[:, 'sii'].notna()]\n",
    "train_df_subset.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "X_train = train_df_subset.loc[:, train_df_subset.columns != 'sii']\n",
    "y = train_df_subset['sii'].astype('Int64')\n",
    "X_test = test_df \n",
    "\n",
    "numerical_cols = data_dict[ (data_dict['Type']=='float') | (data_dict['Type']=='int') ]['Field'].tolist()\n",
    "numerical_cols.remove('PCIAT-PCIAT_Total')\n",
    "numerical_cols = numerical_cols + timeseries_cols\n",
    "\n",
    "categorical_cols = data_dict[ data_dict['Type']=='categorical int' ]['Field'].tolist()\n",
    "categorical_cols = [item for item in categorical_cols if (item not in drop_cols)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this inial preprocessing, the dataset comprises of $241$ numerical (most of which are comprised of time-series statistics extracted from the actigraphy files) and $11$ categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Basic_Demos-Age', 'CGAS-CGAS_Score', 'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU', 'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_PU', 'FGC-FGC_SRL', 'FGC-FGC_SRR', 'FGC-FGC_TL', 'BIA-BIA_BMC', 'BIA-BIA_BMI', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T', 'stat_0', 'stat_1', 'stat_2', 'stat_3', 'stat_4', 'stat_5', 'stat_6', 'stat_7', 'stat_8', 'stat_9', 'stat_10', 'stat_11', 'stat_12', 'stat_13', 'stat_14', 'stat_15', 'stat_16', 'stat_17', 'stat_18', 'stat_19', 'stat_20', 'stat_21', 'stat_22', 'stat_23', 'stat_24', 'stat_25', 'stat_26', 'stat_27', 'stat_28', 'stat_29', 'stat_30', 'stat_31', 'stat_32', 'stat_33', 'stat_34', 'stat_35', 'stat_36', 'stat_37', 'stat_38', 'stat_39', 'stat_40', 'stat_41', 'stat_42', 'stat_43', 'stat_44', 'stat_45', 'stat_46', 'stat_47', 'stat_48', 'stat_49', 'stat_50', 'stat_51', 'stat_52', 'stat_53', 'stat_54', 'stat_55', 'stat_56', 'stat_57', 'stat_58', 'stat_59', 'stat_60', 'stat_61', 'stat_62', 'stat_63', 'stat_64', 'stat_65', 'stat_66', 'stat_67', 'stat_68', 'stat_69', 'stat_70', 'stat_71', 'stat_72', 'stat_73', 'stat_74', 'stat_75', 'stat_76', 'stat_77', 'stat_78', 'stat_79', 'stat_80', 'stat_81', 'stat_82', 'stat_83', 'stat_84', 'stat_85', 'stat_86', 'stat_87', 'stat_88', 'stat_89', 'stat_90', 'stat_91', 'stat_92', 'stat_93', 'stat_94', 'stat_95', 'stat_96', 'stat_97', 'stat_98', 'stat_99', 'stat_100', 'stat_101', 'stat_102', 'stat_103', 'stat_104', 'stat_105', 'stat_106', 'stat_107', 'stat_108', 'stat_109', 'stat_110', 'stat_111', 'stat_112', 'stat_113', 'stat_114', 'stat_115', 'stat_116', 'stat_117', 'stat_118', 'stat_119', 'stat_120', 'stat_121', 'stat_122', 'stat_123', 'stat_124', 'stat_125', 'stat_126', 'stat_127', 'stat_128', 'stat_129', 'stat_130', 'stat_131', 'stat_132', 'stat_133', 'stat_134', 'stat_135', 'stat_136', 'stat_137', 'stat_138', 'stat_139', 'stat_140', 'stat_141', 'stat_142', 'stat_143', 'stat_144', 'stat_145', 'stat_146', 'stat_147', 'stat_148', 'stat_149', 'stat_150', 'stat_151', 'stat_152', 'stat_153', 'stat_154', 'stat_155', 'stat_156', 'stat_157', 'stat_158', 'stat_159', 'stat_160', 'stat_161', 'stat_162', 'stat_163', 'stat_164', 'stat_165', 'stat_166', 'stat_167', 'stat_168', 'stat_169', 'stat_170', 'stat_171', 'stat_172', 'stat_173', 'stat_174', 'stat_175', 'stat_176', 'stat_177', 'stat_178', 'stat_179', 'stat_180', 'stat_181', 'stat_182', 'stat_183', 'stat_184', 'stat_185', 'stat_186', 'stat_187', 'stat_188', 'stat_189', 'stat_190', 'stat_191', 'stat_192', 'stat_193', 'stat_194', 'stat_195', 'stat_196', 'stat_197', 'stat_198', 'stat_199', 'stat_200', 'stat_201', 'stat_202', 'stat_203']\n",
      "There are 241 numerical features.\n"
     ]
    }
   ],
   "source": [
    "print(numerical_cols)\n",
    "print(f'There are {len(numerical_cols)} numerical features.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Basic_Demos-Sex', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL_Zone', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num', 'PreInt_EduHx-computerinternet_hoursday']\n",
      "There are 11 categorical features.\n"
     ]
    }
   ],
   "source": [
    "print(categorical_cols)\n",
    "print(f'There are {len(categorical_cols)} categorical features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is severe imbalance between the different ```sii``` classes, with ```sii``` of $0$ comprising of more than $40\\%$ of the cases in the data. Hence, during cross-validation, stratification will become necessary. Even after stratification, however, the algorithm might not be able to effectively categorize ```sii``` of $3$ since it comprises of such a small proportion of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train_df, x='sii', hue='Basic_Demos-Sex', stat='percent')\n",
    "plt.title('SII classes', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions\n",
    "We define some useful functions here. \n",
    "1. ```clipped(X, r)```: This function takes in the numerical columns of the training data as input and winsorizes only those columns for which the standard deviation is much greater than the mean, expressed as the ratio $r$. This can be an effective way of winsorizing only those features that have very large outliers (some of those in the ```BIA``` category) without fixing them by hand.\n",
    "2. ```process(X, num_cols, cat_cols)```: This function performs a few simple processing steps on the data such as the aforementioned winsorization on the numerical columns and replacing occurrences of zeros with column means and converting the categorical features in ```Int64```, turning them into ordinal variables.\n",
    "3. ```cross_val_QWK(reg_class, X, y, num_cols, cat_cols, cv, verbose=False)```: This function performs a whole host of tasks and is responsible for performing stratified $k$-fold cross-validation and making predictions. Apart from these steps, this function also performs the following - \n",
    "    * **Data preprocessing:** This is done inside the cross-validation loop to prevent data leakage. \n",
    "    * $\\kappa$ **-scores:** The weighted $\\kappa$ scores are calculated for both training and validation sets at each fold using the ```quadratic_weighted_kappa(y_true, y_pred)``` function. The validation set predictions here are labeled ```oof``` (out-of -fold) for wihch there are ```oof_not_rounded``` and ```oof_rounded```, where the former arises from the fact that the model is being trained with regressors. The rounding thresholds can be the default $x_0=\\left[ 0.5, 1.5, 2.5 \\right]$ or a optimal set of thresholds by maximizing $\\kappa$. This optimized threshold $x_{0,\\text{opt}}$ is then used to round-off the mean predictions over the $k$-folds of cross-validation.\n",
    "4. ```objective(trial, reg, X, y, num_cols, cat_cols, cv, verbose=False)``` and ```hyperparam_optim(reg, X, y, num_cols, cat_cols, n_trials=25, cv=5)```: These functions perform hyperparameter optimization using ```Optuna```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped(X, r):\n",
    "    ''' \n",
    "    This function caps the numerical columns to the specified upper and lower quantiles on respective ends. It does so for those featutres\n",
    "    for which the standard deviation is much greater or equal the mean. This is passed as a desired ratio of std/mean.\n",
    "    '''\n",
    "    X_clipped = X.copy()\n",
    "\n",
    "    ratio = X_clipped.std() / X_clipped.mean()\n",
    "    idx_list = ratio[ ratio.where(ratio >= r).notna() ].index.tolist()\n",
    "\n",
    "    for idx in idx_list:\n",
    "        lq = X_clipped[idx].quantile(0.05)\n",
    "        uq = X_clipped[idx].quantile(0.95)\n",
    "        X_clipped.loc[:, idx] = X_clipped.loc[:, idx].clip(lower=lq, upper=uq)\n",
    "\n",
    "    return X_clipped\n",
    "\n",
    "def process(X, num_cols, cat_cols):\n",
    "    ''' \n",
    "    This function performs preprocessing on the data. The training data is first split into numerical and categorical features.\n",
    "    '''\n",
    "    X_numerical = X[num_cols]\n",
    "\n",
    "    X_categorical = X[cat_cols]\n",
    "    X_categorical = X_categorical.astype('Int64')\n",
    "\n",
    "    # Some numerical features have erroneous zero values - e.g. BMI, blood pressure etc\n",
    "    # Winsorize X_numerical and replace occurrences of zero with feature mean\n",
    "\n",
    "    X_numerical = clipped(X_numerical, 1.5)\n",
    "    X_numerical.replace(0, X_numerical.mean(axis=0), inplace=True)\n",
    "\n",
    "    X_processed = X_categorical.join(X_numerical)\n",
    "\n",
    "    return X_processed\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_rounder(oof_not_rounded, thresholds):\n",
    "    ''' \n",
    "    This function iteratively rounds up (or down) the validation set predictions using a predefined threshold passed as an array.\n",
    "    By default, the thresholds can be set as [0.5, 1.5, 2.5], such that predicts lesser (or greater) than the thresholds will be\n",
    "    rounded to the nearest integer.\n",
    "    \n",
    "    This function can be particularly useful when one wishes to implement a custom rounding threshold.\n",
    "    '''\n",
    "    thresh_0, thresh_1, thresh_2 = thresholds # unpack the thresholds\n",
    "\n",
    "    return np.where(\n",
    "        oof_not_rounded < thresh_0, 0, np.where(oof_not_rounded < thresh_1, 1, np.where(oof_not_rounded < thresh_2, 2, 3))\n",
    "    )\n",
    "\n",
    "def evaluate_predictions(thresholds, y, oof_not_rounded):\n",
    "\ty_pred_rounded = threshold_rounder(oof_not_rounded, thresholds)\n",
    "\treturn -quadratic_weighted_kappa(y, y_pred_rounded)\n",
    "\n",
    "def cross_val_QWK(reg_class, X, y, num_cols, cat_cols, cv, verbose=False):\n",
    "    ''' \n",
    "    This function takes in a particular classifier and training data and perfoms Stratified k-Fold cross-validation on it\n",
    "    and calculates the out-of-fold (OOF) QWK score.\n",
    "    '''\n",
    "    X = process(X, num_cols, cat_cols)\n",
    "    N_SPLITS = cv\n",
    "    train_scores = [] # training QWK scores across folds\n",
    "    val_scores = []   # validation QWK scores across folds\n",
    "\n",
    "    oof_not_rounded = np.zeros(len(y), dtype=np.float32) # array for storing out-of-fold prediction from regressor\n",
    "    oof_rounded = np.zeros(len(y), dtype=np.int32) # array for storing out-of-fold prediction that has been rounded\n",
    "    y_pred = np.zeros((len(X_test), N_SPLITS))\n",
    "    \n",
    "    SKF = StratifiedKFold(n_splits=N_SPLITS, shuffle=True)\n",
    "\n",
    "    ''' \n",
    "    Performing Stratified k-fold cross validation with n_splits=5; the training set XX will be further\n",
    "    split into training and validation sets a total of n_split times. The data preprocessing steps (imputation and scaling)\n",
    "    will be applied here to prevent data leakage.\n",
    "    '''\n",
    "    for fold, (train_idx, test_idx) in enumerate(SKF.split(X, y)):\n",
    "\n",
    "        # Breaking up the training data into further training and validation sets during each iteration\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Data pre-processing\n",
    "\n",
    "        numerical_transformer = Pipeline(\n",
    "            steps=[\n",
    "                ('imp_num', KNNImputer(n_neighbors=5, weights='uniform')),\n",
    "                #('imp_num', SimpleImputer(strategy='mean')),\n",
    "                #('ss', StandardScaler())\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[\n",
    "                ('imp_cat', KNNImputer(n_neighbors=5, weights='uniform')),\n",
    "                #('imp_cat', SimpleImputer(strategy='most_frequent')),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('numerical_transforms', numerical_transformer, num_cols),\n",
    "                ('categorical_transforms', categorical_transformer, cat_cols[1:])\n",
    "            ], remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        preprocessor.set_output(transform='pandas')\n",
    "        X_train = preprocessor.fit_transform(X_train)\n",
    "        X_val = preprocessor.transform(X_val)\n",
    "        test_data = preprocessor.transform(X_test)\n",
    "\n",
    "        reg = clone(reg_class)\n",
    "        reg.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = reg.predict(X_train)\n",
    "        y_val_pred = reg.predict(X_val)\n",
    "\n",
    "        oof_not_rounded[test_idx] = y_val_pred\n",
    "        oof_rounded[test_idx] = y_val_pred.round(0).astype('int32')\n",
    "\n",
    "        kappa_train = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype('int32'))\n",
    "        kappa_val = quadratic_weighted_kappa(y_val, y_val_pred.round(0).astype('int32'))\n",
    "\n",
    "        train_scores.append(kappa_train)\n",
    "        val_scores.append(kappa_val)\n",
    "        y_pred[:, fold] = reg.predict(test_data)\n",
    "\n",
    "        if verbose:\n",
    "\n",
    "            print(f'Fold {fold + 1}: Training QWK = {kappa_train:.4f}, Validation QWK: {kappa_val:.4f}')\n",
    "\n",
    "    if verbose:\n",
    "\n",
    "        print(f'Mean training QWK = {np.mean(train_scores):.4f}')\n",
    "        print(f'Mean validation QWK = {np.mean(val_scores):.4f}')\n",
    "\n",
    "    ''' \n",
    "    In kappa_optimizer the QWK metric is maximized (or rather the negative QWK is minimize) to obtain a set of optimal \n",
    "    rounding threshold values.\n",
    "    '''\n",
    "    kappa_optimizer = minimize(evaluate_predictions, x0 = [0.5, 1.5, 2.5], args=(y, oof_not_rounded), method='Nelder-Mead')\n",
    "    threshold_optim = kappa_optimizer.x\n",
    "    #assert kappa_optimizer.succes, 'Optimizer did not converge.'\n",
    "\n",
    "    oof_tuned = threshold_rounder(oof_not_rounded, threshold_optim)\n",
    "    kappa_val_tuned = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f'Tuned QWK = {kappa_val_tuned}')\n",
    "\n",
    "    y_pred_mean = threshold_rounder(y_pred.mean(axis=1), threshold_optim)\n",
    "\n",
    "    return y_pred_mean \n",
    "    \n",
    "\n",
    "def objective(trial, reg, X, y, num_cols, cat_cols, cv, verbose=False):\n",
    "    ''' \n",
    "    This function performs hyperparameter optimization using Optuna for a given classifier model.\n",
    "    '''\n",
    "\n",
    "    if reg == 'XGBoost':\n",
    "        # Hyperparameters for XGBoost regressor\n",
    "        params = {\n",
    "            'objective' : trial.suggest_categorical('objective', ['reg:squarederror', 'reg:absoluteerror', 'reg:pseudohubererror']),\n",
    "            'n_estimators' : trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 5, 15),\n",
    "            'learning_rate' : trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "            'gamma' : trial.suggest_float('gamma', 0.0, 5.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 0.1, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 0.1, log=True),\n",
    "            'subsample' : trial.suggest_float('subsample', 0.5, 1.0)\n",
    "        }\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "\n",
    "    elif reg == 'LightGBM':\n",
    "        # Hyperparameters for LightGBM regressor\n",
    "        params = {\n",
    "            'objective' : trial.suggest_categorical('objective', ['regression', 'poisson', 'quantile']),\n",
    "            'verbosity' : -1,\n",
    "            'n_estimators' : trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 5, 15),\n",
    "            'learning_rate' : trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),\n",
    "            'subsample' : trial.suggest_float('subsample', 0.5, 1.0)\n",
    "        }\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "\n",
    "    elif reg == 'CatBoost':\n",
    "        # Hyperparameters for CatBoost regressor\n",
    "        params = {\n",
    "            'objective' : trial.suggest_categorical('objective', ['RMSE', 'Poisson', 'Quantile']),\n",
    "            'iterations': trial.suggest_int('iterations', 200, 500),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-5, 0.1, log=True),\n",
    "            #'subsample': trial.suggest_float('subsample', 0.5, 1.0)\n",
    "        }\n",
    "\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(f'Unsupported regressor type: {reg}')\n",
    "    \n",
    "    val_QWK = cross_val_QWK(model, X, y, num_cols, cat_cols, cv)\n",
    "\n",
    "    return val_QWK\n",
    "\n",
    "def hyperparam_optim(reg, X, y, num_cols, cat_cols, n_trials=25, cv=5): \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, reg, X, y, num_cols, cat_cols, cv),\n",
    "        n_trials=n_trials \n",
    "    )\n",
    "\n",
    "    print(f'Best parameters for {reg}: {study.best_params}')\n",
    "    print(f'Best QWK score: {study.best_value}')\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Training QWK = 0.7567, Validation QWK: 0.3975\n",
      "Fold 2: Training QWK = 0.7621, Validation QWK: 0.3876\n",
      "Fold 3: Training QWK = 0.7534, Validation QWK: 0.3323\n",
      "Fold 4: Training QWK = 0.7595, Validation QWK: 0.4193\n",
      "Fold 5: Training QWK = 0.7547, Validation QWK: 0.3425\n",
      "Mean training QWK = 0.7573\n",
      "Mean validation QWK = 0.3758\n",
      "Tuned QWK = 0.4491222179082237\n"
     ]
    }
   ],
   "source": [
    " # validation QWK = 0.369\n",
    "xgb_params = {\n",
    "    'objective': 'reg:pseudohubererror',\n",
    "    'n_estimators': 157,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03747807982154853,\n",
    "    'gamma': 3.3972493317616235,\n",
    "    'reg_alpha': 0.019288617659988346,\n",
    "    'reg_lambda': 1.1622365647503362e-05,\n",
    "    'subsample': 0.5463994889151166\n",
    "}\n",
    "\n",
    "# validation QWK = 0.394\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'n_estimators': 438,\n",
    "    'max_depth': 12,\n",
    "    'learning_rate': 0.011007625265216012,\n",
    "    'subsample': 0.9500505257332115\n",
    "}\n",
    "\n",
    "# validation QWK = 0.382\n",
    "cb_params = {\n",
    "    'objective': 'Poisson',\n",
    "    'iterations': 410,\n",
    "    'depth': 4,\n",
    "    'learning_rate': 0.03217561086495608,\n",
    "    'l2_leaf_reg': 0.0030777935770974554\n",
    "}\n",
    "\n",
    "xgb_reg = XGBRegressor(**xgb_params)\n",
    "lgb_reg = LGBMRegressor(**lgb_params, verbosity=-1)\n",
    "cb_reg = CatBoostRegressor(**cb_params, verbose=0)\n",
    "\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('XGBoost', xgb_reg),\n",
    "        ('LightGBM', lgb_reg),\n",
    "        ('CatBoost', cb_reg)\n",
    "    ],\n",
    "    weights = [1, 1, 1]\n",
    ")\n",
    "\n",
    "y_pred = cross_val_QWK(voting_reg, X_train, y, numerical_cols, categorical_cols, cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': X_test.index.tolist(), 'sii': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
